{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global ROOT_DIR\n",
    "ROOT_DIR = '/gpfs/commons/groups/gursoy_lab/aelhussein/ot_cost/otcost_fl_rebase'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(f'{ROOT_DIR}/code/helper')\n",
    "import OTCost as ot\n",
    "import importlib\n",
    "importlib.reload(ot)\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "import copy\n",
    "from emnist import extract_training_samples\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset, cost):\n",
    "    if dataset == 'Synthetic':\n",
    "        cost = \"{:.2f}\".format(cost)\n",
    "        d1 = pd.read_csv(f'{ROOT_DIR}/data/{dataset}/data_1_{cost}.csv',sep = ' ', names = [i for i in range(13)])\n",
    "        d2 = pd.read_csv(f'{ROOT_DIR}/data/{dataset}/data_2_{cost}.csv', sep = ' ', names = [i for i in range(13)])\n",
    "        X1, y1 = d1.iloc[:,:-1].values, d1.iloc[:,-1].values\n",
    "        X2, y2 = d2.iloc[:,:-1].values, d2.iloc[:,-1].values\n",
    "    \n",
    "    elif dataset == 'Credit':\n",
    "        cost = \"{:.2f}\".format(cost)\n",
    "        d1 = pd.read_csv(f'{ROOT_DIR}/data/{dataset}/data_1_{cost}.csv',sep = ' ', names = [i for i in range(29)])\n",
    "        d2 = pd.read_csv(f'{ROOT_DIR}/data/{dataset}/data_2_{cost}.csv', sep = ' ', names = [i for i in range(29)])\n",
    "        d1 = d1.sample(n=500)\n",
    "        d2 = d2.sample(n=500)\n",
    "        X1, y1 = d1.iloc[:,:-1].values, d1.iloc[:,-1].values\n",
    "        X2, y2 = d2.iloc[:,:-1].values, d2.iloc[:,-1].values\n",
    "    \n",
    "    elif dataset == 'Weather':\n",
    "        X1,  y1,  X2, y2 = load_data_weather(cost)\n",
    "\n",
    "    elif dataset == 'EMNIST':\n",
    "        cost = \"{:.2f}\".format(cost)\n",
    "        d1 = np.load(f'{ROOT_DIR}/data/{dataset}/data_1_{cost}.npz')\n",
    "        d2 = np.load(f'{ROOT_DIR}/data/{dataset}/data_2_{cost}.npz')\n",
    "\n",
    "        X1, y1 = d1['data'], d1['labels']\n",
    "        idx = np.random.choice(np.arange(X1.shape[0]), 500, replace=False)\n",
    "        X1, y1 = X1[idx], y1[idx]\n",
    "        X1 = X1.reshape(28*28,-1)\n",
    "\n",
    "        X2, y2 = d2['data'], d2['labels']\n",
    "        idx = np.random.choice(np.arange(X2.shape[0]), 500, replace=False)\n",
    "        X2, y2 = X2[idx], y2[idx]\n",
    "        X2 = X2.reshape(28*28,-1)\n",
    "\n",
    "    elif dataset == 'CIFAR':\n",
    "        cost = \"{:.2f}\".format(cost)\n",
    "        X1, X2, y1, y2 = load_cifar_embeddings(cost)\n",
    "\n",
    "    elif dataset == 'IXITiny':\n",
    "        X1 = load_data_ixitiny(cost[0])\n",
    "        X2 = load_data_ixitiny(cost[1])\n",
    "        return {\"1\":X1, \"2\":X2}\n",
    "    \n",
    "    elif dataset == 'ISIC':\n",
    "        X1, y1 = load_data_isic(cost[0])\n",
    "        X2, y2 = load_data_isic(cost[1])\n",
    "        \n",
    "    return {\"1\":X1, \"2\":X2}, {\"1\":y1, \"2\":y2}\n",
    "\n",
    "def load_data_weather(costs):\n",
    "     weather_df = load_all_weather()\n",
    "     return dictionaryCreator(weather_df, costs, n = 5000)\n",
    "\n",
    "def load_all_weather():\n",
    "    DATA_DIR = f'{ROOT_DIR}/data/Weather'\n",
    "    ##load dataset\n",
    "    df = pd.read_csv(f'{DATA_DIR}/shifts_canonical_train.csv', nrows = 20000)\n",
    "    df[((df['climate'] == 'tropical') & (df['fact_temperature'] > 25)) | \n",
    "        ((df['climate'] == 'mild temperate') & ((df['fact_temperature'] > 10) & (df['fact_temperature'] < 25))) |\n",
    "        (df['climate'] == 'dry') & ((df['fact_temperature'] > 5) & (df['fact_temperature'] < 25))]\n",
    "    df_snow = pd.read_csv(f'{DATA_DIR}/shifts_canonical_eval_out.csv', nrows = 20000)\n",
    "    df_snow = df_snow[df_snow['fact_temperature'] < 10]\n",
    "    df = pd.concat([df, df_snow])\n",
    "    df.dropna(inplace = True)\n",
    "    return df\n",
    "\n",
    "def extractData(df, climate, n):\n",
    "    df = df[df['climate'].isin(climate)]\n",
    "    ind = np.random.choice(df.shape[0], n)\n",
    "    X = df.iloc[ind, 6:]\n",
    "    y = df.iloc[ind, 5] # climate as the label for ot cosr calculation\n",
    "    return X.values, y.values\n",
    "\n",
    "def dictionaryCreator(df, climates, n = 500):\n",
    "    ##wrangle to dictionary for OT cost calculation\n",
    "    X1, y1 = extractData(df, climates[0],n)\n",
    "    scaler = StandardScaler()\n",
    "    X1_normalized = scaler.fit_transform(X1)   \n",
    "    X2, y2 = extractData(df, climates[1],n)\n",
    "    X2_normalized = scaler.transform(X2)  \n",
    "   \n",
    "    return X1_normalized,  y1,  X2_normalized, y2\n",
    "\n",
    "\n",
    "class EmbeddedImagesDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, embedding, label, coarse_label = self.data[idx]\n",
    "        return image, embedding, label, coarse_label\n",
    "    \n",
    "def extract_by_labels(dataset, target_labels, label_type = 'fine'):\n",
    "    extracted_data = []\n",
    "    for image, embedding, fine_label, coarse_label in dataset:\n",
    "        if label_type == 'fine':\n",
    "            label = fine_label\n",
    "        else:\n",
    "            label = coarse_label\n",
    "        if label in target_labels:\n",
    "            extracted_data.append((image, embedding, fine_label, coarse_label))\n",
    "    return EmbeddedImagesDataset(extracted_data)\n",
    "\n",
    "def get_datasets(dataset, labels_extract, label_type = 'fine'):\n",
    "    d1 = extract_by_labels(dataset, labels_extract[0], label_type)\n",
    "    d2 = extract_by_labels(dataset, labels_extract[1], label_type)\n",
    "    return d1, d2\n",
    "\n",
    "def sampler(dataset, num_samples):\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    sampled_data = [dataset[i] for i in indices]\n",
    "    embs = np.array([entry[1] for entry in sampled_data])\n",
    "    label = np.array([entry[2] for entry in sampled_data])\n",
    "    return embs, label\n",
    "\n",
    "def load_cifar_embeddings(cost):\n",
    "    labels = {'0.08': [[x for x in range(10)], [x for x in range(10)]],\n",
    "            '0.21': [[11,98,29,73, 78, 49, 97, 51, 55, 92], [11,98,29,73, 78, 49, 42, 83, 72, 82]],\n",
    "            '0.30':[[11,50,78,1,92, 78, 49, 97, 55, 16, 14], [11, 36, 29, 73, 82, 78, 49, 42, 12, 23, 51]],\n",
    "            '0.38':[[11,50,78,8,92,2,49,98,89,3], [17, 36, 30, 73, 83,28, 34, 42, 10, 20]]}\n",
    "            \n",
    "    labels_extract = labels[cost]\n",
    "    with open(f'{ROOT_DIR}/data/CIFAR/cifar_{1000}_emb.pkl', 'rb') as f:\n",
    "        data= pickle.load(f)\n",
    "    d1, d2 = get_datasets(data, labels_extract)\n",
    "    num_samples = 500\n",
    "    X1, y1 =  sampler(d1, num_samples)\n",
    "    X2, y2 =  sampler(d2, num_samples)\n",
    "    return X1, X2, y1, y2\n",
    "\n",
    "def load_data_ixitiny(site):\n",
    "    files = os.listdir(f'{ROOT_DIR}/data/IXITiny/embedding')\n",
    "    files_site = [file for file in files if site in file]\n",
    "    embeddings = []\n",
    "    for file in files_site:\n",
    "        emb = np.load(f'{ROOT_DIR}/data/IXITiny/embedding/{file}')\n",
    "        embeddings.append(emb)\n",
    "    embeddings = np.array(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def load_data_isic(site):\n",
    "    files = pd.read_csv(f'{ROOT_DIR}/data/ISIC/site_{site}_files_used.csv')\n",
    "    files = files.sample(500)\n",
    "    label_counts = files['label'].value_counts()\n",
    "    selected_labels = label_counts[label_counts > 30].index\n",
    "    filtered_files = files[files['label'].isin(selected_labels)]\n",
    "    files_used  = list(filtered_files['image'].values)\n",
    "    labels_used = list(filtered_files['label'].values)\n",
    "    embeddings = []\n",
    "    for file in files_used:\n",
    "        emb = np.load(f'{ROOT_DIR}/data/ISIC/embedding/center_{site}_{file}.npy')\n",
    "        embeddings.append(emb)\n",
    "    embeddings = np.array(embeddings)\n",
    "    return embeddings, np.array(labels_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grapher(dataset, df, save = True):\n",
    "    for i in range(1, df.shape[0]):\n",
    "        sns.lineplot( x= df.iloc[0], y =df.iloc[i], alpha = 0.8, marker = 'o', label = f'{df.index[i][0]}:{df.index[i][1]}')\n",
    "    plt.plot(np.linspace(0.1, 0.42, 100), np.linspace(0.1, 0.42, 100), linestyle = '--', alpha = 0.5, label = 'Perfect agreement', color = 'black')\n",
    "    plt.xlabel('cost at lambda =2:1', fontsize = 12)\n",
    "    plt.ylabel(f'Calculated cost', fontsize = 12)\n",
    "    plt.legend(bbox_to_anchor=(1, 1), title=\"lambda\")\n",
    "    plt.xticks()\n",
    "    plt.yticks()\n",
    "    if save == True:\n",
    "        plt.savefig(f'{ROOT_DIR}/results/lambda_param/{dataset}_lambda_param.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ot)\n",
    "dataset = 'Synthetic'\n",
    "costs = [0.03, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "lams = [(2,1), (1,1), (1,2), (3,1), (1,3)]\n",
    "results_synthetic = {}\n",
    "for cost in costs:\n",
    "    data, label= load_data(dataset, cost)\n",
    "    results_synthetic[cost] = []\n",
    "    for lam in lams:\n",
    "        ## calculate cost        \n",
    "        OTCost_label = ot.OTCost(dataset, data, label, lam = lam)\n",
    "        c = OTCost_label.calculate_ot_cost()\n",
    "        results_synthetic[cost].append(c)\n",
    "\n",
    "df = pd.DataFrame(results_synthetic, index = lams)\n",
    "grapher(dataset, df, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ot)\n",
    "costs = [0.12, 0.23, 0.30, 0.40]\n",
    "dataset = 'Credit'\n",
    "lams = [(2,1), (1,1), (1,2), (3,1), (1,3)]\n",
    "results_credit = {}\n",
    "for cost in costs:\n",
    "    data, label= load_data(dataset, cost)\n",
    "    results_credit[cost] = []\n",
    "    for lam in lams:\n",
    "        ## calculate cost        \n",
    "        OTCost_label = ot.OTCost(dataset, data, label,  lam = lam)\n",
    "        c = OTCost_label.calculate_ot_cost()\n",
    "        results_credit[cost].append(c)\n",
    "\n",
    "df = pd.DataFrame(results_credit, index = lams)\n",
    "grapher(dataset, df, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ot)\n",
    "dataset = 'Weather'\n",
    "lams = [(2,1), (1,1), (1,2), (3,1), (1,3)]\n",
    "climates = [[['tropical', 'mild temperate'],['tropical', 'mild temperate']],\n",
    "            [['tropical', 'mild temperate'], ['dry', 'mild temperate']],\n",
    "            [['tropical', 'mild temperate'], ['dry']],\n",
    "            [['tropical', 'mild temperate'], ['snow', 'dry']],\n",
    "            [['tropical', 'mild temperate'],['snow']]]\n",
    "results_weather = {}\n",
    "for cost, climate in zip(costs, climates):\n",
    "    data, label= load_data(dataset, climate)\n",
    "    results_weather[cost] = []\n",
    "    for lam in lams:\n",
    "        ## calculate cost        \n",
    "        OTCost_label = ot.OTCost(dataset, data, label,  lam = lam)\n",
    "        c = OTCost_label.calculate_ot_cost()\n",
    "        results_weather[cost].append(c)\n",
    "\n",
    "df = pd.DataFrame(results_weather, index = lams)\n",
    "grapher(dataset, df, save = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ot)\n",
    "costs = [0.11,0.19,0.25,0.34,0.39]\n",
    "dataset = 'EMNIST'\n",
    "lams = [(2,1), (1,1), (1,2), (3,1), (1,3)]\n",
    "results_emnist = {}\n",
    "for cost in costs:\n",
    "    data, label= load_data(dataset, cost)\n",
    "    results_emnist[cost] = []\n",
    "    for lam in lams:\n",
    "        ## calculate cost        \n",
    "        OTCost_label = ot.OTCost(dataset, data, label,  lam = lam)\n",
    "        c = OTCost_label.calculate_ot_cost()\n",
    "        results_emnist[cost].append(c)\n",
    "\n",
    "df = pd.DataFrame(results_emnist, index = lams)\n",
    "grapher(dataset, df, save = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ot)\n",
    "costs = [0.08, 0.21, 0.30, 0.38]\n",
    "indices =  [\n",
    "            [[x for x in range(10)], [x for x in range(10)]],\n",
    "            [[11,98,29,73, 78, 49, 97, 51, 55, 92], [11,98,29,73, 78, 49, 42, 83, 72, 82]],\n",
    "            [[11,50,78,1,92, 78, 49, 97, 55, 16, 14], [11, 36, 29, 73, 82, 78, 49, 42, 12, 23, 51]],\n",
    "            [[11,50,78,8,92,2,49,98,89,3], [17, 36, 30, 73, 83,28, 34, 42, 10, 20]]\n",
    "            ]\n",
    "dataset = 'CIFAR'\n",
    "lams = [1e-3, 5e-3, 1e-2, 5e-2, 5e-1, 5, 50]\n",
    "results_cifar = {}\n",
    "for ind, cost in zip(indices, costs):\n",
    "    d1, d2 = get_datasets(data,ind)\n",
    "    x1, y1 =  sampler(d1)\n",
    "    x2, y2 =  sampler(d2)\n",
    "    results_cifar[cost] = []\n",
    "    for lam in lams:\n",
    "        ## calculate cost        \n",
    "        OTCost_label = ot.OTCost(dataset, data, label,  lam = lam)\n",
    "        c = OTCost_label.calculate_ot_cost()\n",
    "        results_cifar[cost].append(c)\n",
    "\n",
    "df = pd.DataFrame(results_cifar, index = lams)\n",
    "grapher(dataset, df, save = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'IXITiny'\n",
    "DATA_DIR = f'{ROOT_DIR}/data/{DATASET}'\n",
    "files = os.listdir(f'{DATA_DIR}/embedding')\n",
    "sites = ['Guys', 'HH', 'IOP']\n",
    "site_samples = {}\n",
    "site_embeddings = {}\n",
    "for site in sites:\n",
    "    site_samples[site] = [file for file in files if site in file]\n",
    "\n",
    "for site, files in site_samples.items():\n",
    "    embeddings = [np.load(f'{DATA_DIR}/embedding/{file}') for file in files]\n",
    "    site_embeddings[site] = np.array(embeddings) \n",
    "\n",
    "\n",
    "importlib.reload(ot)\n",
    "costs = [0.08, 0.21, 0.30, 0.38]\n",
    "indices =  [['Guys', 'HH'],\n",
    "            ['Guys', 'IOP'],\n",
    "            ['HH', 'IOP']]\n",
    "dataset = 'IXITiny'\n",
    "lams = [1e-3, 5e-3, 1e-2, 5e-2, 5e-1, 5, 50]\n",
    "results_ixitiny = {}\n",
    "for ind, cost in zip(indices, costs):\n",
    "    X1, X2 = site_embeddings[ind[0]][:], site_embeddings[ind[1]][:]\n",
    "    data = {'1': X1, '2': X2}\n",
    "    labels = {'1': np.ones(X1.shape[0]), '2': np.ones(X2.shape[0])}\n",
    "    results_ixitiny[cost] = []\n",
    "    for lam in lams:\n",
    "        ## calculate cost        \n",
    "        OTCost_label = ot.OTCost(dataset, data, label,  lam = lam)\n",
    "        c = OTCost_label.calculate_ot_cost()\n",
    "        results_ixitiny[cost].append(c)\n",
    "\n",
    "df = pd.DataFrame(results_ixitiny, index = lams)\n",
    "grapher(dataset, df, save = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ISIC'\n",
    "DATA_DIR = f'{ROOT_DIR}/data/{DATASET}'\n",
    "##Load labels\n",
    "labels = pd.read_csv(f'{DATA_DIR}/ISIC_2019_Training_GroundTruth.csv')\n",
    "def create_category(row):\n",
    "    for idx, value in enumerate(row):\n",
    "        if value == 1:\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "labels['label'] = labels.apply(create_category, axis=1) - 1\n",
    "labels = labels[['image', 'label']]\n",
    "labels.set_index('image', inplace = True)\n",
    "\n",
    "NUM_SAMPLES = 2000\n",
    "files = os.listdir(f'{DATA_DIR}/embedding')\n",
    "sites = [i for i in range(6)]\n",
    "site_samples = {}\n",
    "site_embeddings = {}\n",
    "site_labels = {}\n",
    "for site in sites:\n",
    "    sites_files = [file for file in files if f'center_{site}' in file]\n",
    "    sites_files = np.random.choice(sites_files, size = NUM_SAMPLES)\n",
    "    site_samples[site] = sites_files\n",
    "    names = [f.split(f'center_{site}_')[-1].split('.npy')[0] for f in sites_files]\n",
    "    labels_site = labels.loc[names]\n",
    "    site_labels[site] = labels_site['label'].values\n",
    "\n",
    "for site, files in site_samples.items():\n",
    "    embeddings = [np.load(f'{DATA_DIR}/embedding/{file}') for file in files]\n",
    "    site_embeddings[site] = np.array(embeddings)\n",
    "\n",
    "def create_dictionaries(site_embeddings, site_labels, sites, NUM_SAMPLES = 500):\n",
    "    data = {'1': site_embeddings[sites[0]][:NUM_SAMPLES], '2' :site_embeddings[sites[1]][:NUM_SAMPLES]}\n",
    "    labels = {'1': site_labels[sites[0]][:NUM_SAMPLES], '2': site_labels[sites[1]][:NUM_SAMPLES]}\n",
    "    data, labels = remove_rare_labels(data, labels, min_count = 30)\n",
    "    return data, labels\n",
    "\n",
    "#Function is needed as estimating label cost with fewer data points leads to degeneracy\n",
    "def remove_rare_labels(data, labels, min_count):\n",
    "    for key in labels:\n",
    "        unique_labels, counts = np.unique(labels[key], return_counts=True)\n",
    "        labels_to_remove = unique_labels[counts <= min_count]\n",
    "        mask = np.isin(labels[key], labels_to_remove, invert=True)\n",
    "        labels[key] = labels[key][mask]\n",
    "        data[key] = data[key][mask]\n",
    "    return data, labels\n",
    "\n",
    "importlib.reload(ot)\n",
    "site_pairs = [(2,2), (2,0), (2,3), (2,1), (1,3)]\n",
    "results = {}\n",
    "nums = [200, 400, 600, 800, 999]\n",
    "dataset = 'ISIC'\n",
    "lams = [1e-3, 5e-3, 1e-2, 5e-2, 5e-1, 5, 50]\n",
    "results_isic = {}\n",
    "for ind, cost in zip(site_pairs, costs):\n",
    "    results_isic[cost] = {}\n",
    "    for lam in lams:\n",
    "        importlib.reload(ot)\n",
    "        if ind[0] != ind[1]:\n",
    "            data, labels = create_dictionaries(site_embeddings, site_labels, ind)\n",
    "            ISIC_OTCost_label = ot.OTCost(DATASET, data, labels)\n",
    "            cost = ISIC_OTCost_label.calculate_ot_cost()\n",
    "            results_isic[cost].append(c)\n",
    "        else:\n",
    "            NUM_SAMPLES = 500\n",
    "            data = {'1': site_embeddings[ind[0]][:NUM_SAMPLES], '2' :site_embeddings[ind[0]][NUM_SAMPLES:NUM_SAMPLES*2]}\n",
    "            labels = {'1': site_labels[ind[0]][:NUM_SAMPLES], '2': site_labels[ind[0]][NUM_SAMPLES:NUM_SAMPLES*2]}\n",
    "            data, labels = remove_rare_labels(data, labels, min_count = 30)\n",
    "            OTCost_label = ot.OTCost(DATASET, data, labels)\n",
    "            cost = OTCost_label.calculate_ot_cost()\n",
    "            results_isic[cost].append(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "281ef65c9cd9845e433bbe5d993e687d8475d676fe0ca3b169ee321bb12e4821"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
