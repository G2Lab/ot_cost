{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global ROOT_DIR\n",
    "ROOT_DIR = '/gpfs/commons/groups/gursoy_lab/aelhussein/ot_cost/otcost_fl_rebase'\n",
    "global DATA_DIR\n",
    "DATA_DIR = f'{ROOT_DIR}/data/ISIC'\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "sys.path.append(f'{ROOT_DIR}/code/ISIC/')\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import dataset\n",
    "from torch.utils.data import DataLoader as dl\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import copy\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LR = 5e-2\n",
    "EPOCHS = 50\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.n_emb = n_emb\n",
    "        self.bottleneck_dim = (16,25,25)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.LeakyReLU(0.1), nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.LeakyReLU(0.1), nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.LeakyReLU(0.1), nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.LeakyReLU(0.1), nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Conv2d(128, 64, 3, padding=1), nn.LeakyReLU(0.1), nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 32, 3, padding=1), nn.LeakyReLU(0.1), nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Conv2d(32, 16, 3, padding=1), nn.LeakyReLU(0.1), nn.BatchNorm2d(16),\n",
    ")\n",
    "        \n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(self.bottleneck_dim[0] * self.bottleneck_dim[1] * self.bottleneck_dim[2], self.n_emb)\n",
    ")\n",
    "        self.expand = nn.Sequential(nn.Linear(self.n_emb, self.bottleneck_dim[0] * self.bottleneck_dim[1] * self.bottleneck_dim[2]))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.LeakyReLU(0.1), nn.BatchNorm2d(32),      \n",
    "            nn.ConvTranspose2d(32, 32, 4, stride=2, padding=1), nn.LeakyReLU(0.1), nn.BatchNorm2d(32),   \n",
    "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1), nn.LeakyReLU(0.1), nn.BatchNorm2d(16),   \n",
    "            nn.ConvTranspose2d(16, 16, 4, stride=2, padding=1), nn.LeakyReLU(0.1), nn.BatchNorm2d(16), \n",
    "            nn.Conv2d(16, 3, 3, padding=1),    \n",
    "            nn.Sigmoid()\n",
    ")\n",
    "    \n",
    "\n",
    "    def forward(self, x, get_embedding=False):\n",
    "        x = self.encoder(x)\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        embedding = self.bottleneck(x_flat)\n",
    "        if get_embedding:\n",
    "            return embedding\n",
    "        x = self.expand(embedding)\n",
    "        x = x.view(x.size(0), self.bottleneck_dim[0], self.bottleneck_dim[1], self.bottleneck_dim[2])\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_autoencoder(n_emb):\n",
    "    # Initialize\n",
    "    model = Autoencoder(n_emb)\n",
    "    model.to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    lr_scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "    \n",
    "    # Load data\n",
    "    train_data = dataset.FedIsic2019(train=True, pooled = True, data_path=DATA_DIR)\n",
    "    \n",
    "\n",
    "    val_data = dataset.FedIsic2019(train=False, pooled = True, data_path=DATA_DIR)\n",
    "    \n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = 10\n",
    "    early_stopping_counter = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Loss tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    subset_batches_train = 200\n",
    "    subset_batches_val = 100\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        #inside loop as i subset the training per epoch\n",
    "        train_loader = dl(train_data, batch_size = BATCH_SIZE, shuffle = True)\n",
    "        val_loader = dl(val_data, batch_size = BATCH_SIZE, shuffle = True)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # Training step\n",
    "        train_loss = 0.0\n",
    "        for i, (image, label) in enumerate(train_loader):\n",
    "            if i>= subset_batches_train:\n",
    "                break\n",
    "            image = image.transpose(2,1)\n",
    "            image = image.to(DEVICE) \n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(image)\n",
    "            loss = criterion(reconstructed, image)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        lr_scheduler.step()   \n",
    "        train_losses.append(train_loss / subset_batches_train)\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        if epoch % 10 == 0:\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i, (image, label) in enumerate(val_loader):\n",
    "                    if i >= subset_batches_val:\n",
    "                        break\n",
    "                    image = image.transpose(2,1)\n",
    "                    image = image.to(DEVICE)\n",
    "                    reconstructed = model(image)\n",
    "                    loss = criterion(reconstructed, image)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "            val_loss /= subset_batches_val\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            print(f'Epoch {epoch}, Train Loss: {train_loss / len(train_loader)}, Validation Loss: {val_loss}')\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_model = copy.deepcopy(model)\n",
    "                best_val_loss = val_loss\n",
    "                early_stopping_counter = 0\n",
    "                best_model.to('cpu')\n",
    "                torch.save(best_model.state_dict(), f'{ROOT_DIR}/data/ISIC/model_checkpoint_{n_emb}.pth')\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "    \n",
    "    best_model.to('cpu')\n",
    "    torch.save(best_model.state_dict(), f'{ROOT_DIR}/data/ISIC/model_checkpoint_{n_emb}.pth')\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def main(n_emb):\n",
    "    return n_emb, train_autoencoder(n_emb)\n",
    "\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    n_embs = [512, 1024, 2048, 4096]\n",
    "    cpu = int(os.environ.get('SLURM_CPUS_PER_TASK', 5))\n",
    "    if DEVICE == 'cpu':\n",
    "        with Pool(cpu) as pool:\n",
    "            results = pool.map(main, n_embs)\n",
    "    else:\n",
    "        results = []\n",
    "        for n_emb in n_embs:\n",
    "            results.append(main(n_emb))\n",
    "        \n",
    "    losses = {}\n",
    "    for n_emb, loss in results:\n",
    "        losses[n_emb] = loss\n",
    "    with open(f'{ROOT_DIR}/data/ISIC/losses.json', 'w') as f:\n",
    "        json.dump(losses, f)\n",
    "    print(\"Losses saved to file.\")\n",
    "'''\n",
    "pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb = 1024\n",
    "# Initialize\n",
    "model = Autoencoder(n_emb)\n",
    "model.to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "lr_scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_data = dataset.FedIsic2019(train=True, pooled = True, data_path=DATA_DIR)\n",
    "val_data = dataset.FedIsic2019(train=False, pooled = True, data_path=DATA_DIR)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "early_stopping_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Loss tracking\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "subset_batches_train = 50\n",
    "subset_batches_val = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.59375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = dl(train_data, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader = dl(val_data, batch_size = BATCH_SIZE, shuffle = True)\n",
    "image, label =  next(iter(train_loader))\n",
    "image = image.transpose(2,1)\n",
    "def get_memory(image):\n",
    "    shape  = image.shape\n",
    "    return shape[0]*shape[1]*shape[2]*shape[3]*4 / (1024**2)\n",
    "get_memory(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 200, 200]), torch.Size([128, 3, 200, 200]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed = model(image)\n",
    "reconstructed.shape, image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    #inside loop as i subset the training per epoch\n",
    "    train_loader = dl(train_data, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    val_loader = dl(val_data, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Training step\n",
    "    train_loss = 0.0\n",
    "    for i, (image, label) in enumerate(train_loader):\n",
    "        if i>= subset_batches_train:\n",
    "            break\n",
    "        image = image.transpose(2,1)\n",
    "        image = image.to(DEVICE) \n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = model(image)\n",
    "        loss = criterion(reconstructed, image)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        lr_scheduler.step()   \n",
    "    train_losses.append(train_loss / subset_batches_train)\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    if epoch % 10 == 0:\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (image, label) in enumerate(val_loader):\n",
    "                if i >= subset_batches_val:\n",
    "                    break\n",
    "                image = image.transpose(2,1)\n",
    "                image = image.to(DEVICE)\n",
    "                reconstructed = model(image)\n",
    "                loss = criterion(reconstructed, image)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= subset_batches_val\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss / len(train_loader)}, Validation Loss: {val_loss}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            best_model.to('cpu')\n",
    "            torch.save(best_model.state_dict(), f'{ROOT_DIR}/data/ISIC/model_checkpoint_{n_emb}.pth')\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "best_model.to('cpu')\n",
    "torch.save(best_model.state_dict(), f'{ROOT_DIR}/data/ISIC/model_checkpoint_{n_emb}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
