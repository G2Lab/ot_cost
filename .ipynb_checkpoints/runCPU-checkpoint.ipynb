{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/gpfs/commons/groups/gursoy_lab/aelhussein/ot_cost/otcost_fl_rebase'\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(f'{ROOT_DIR}/code/helper')\n",
    "import pipeline as pp\n",
    "import trainers as tr\n",
    "import process_results as pr\n",
    "import data_preprocessing as dp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "importlib.reload(pp)\n",
    "importlib.reload(dp)\n",
    "importlib.reload(pr)\n",
    "importlib.reload(tr)\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from sklearn import metrics\n",
    "from scipy.stats import bootstrap\n",
    "from torchvision import models\n",
    "from unet import UNet\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import resample\n",
    "import nibabel as nib\n",
    "import torchio as tio\n",
    "from torch.utils.data  import DataLoader, random_split, TensorDataset, Dataset\n",
    "from torch.nn.modules.loss import _Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUEEZE = ['Synthetic', 'Credit']\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(metric_name):\n",
    "    metric_mapping = {\n",
    "        'Synthetic': metrics.roc_auc_score,\n",
    "        'Credit': metrics.average_precision_score,\n",
    "        'Weather': metrics.r2_score}\n",
    "    return metric_mapping[metric_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(DATASET, data_num, cost):\n",
    "    if DATASET == 'Synthetic':\n",
    "        ##load data\n",
    "        X = pd.read_csv(f'{ROOT_DIR}/data/{DATASET}/data_{data_num}_{cost:.2f}.csv', sep = ' ', names = [i for i in range(13)])\n",
    "        X = X.sample(200)\n",
    "    elif DATASET == 'Credit':\n",
    "        X = pd.read_csv(f'{ROOT_DIR}/data/{DATASET}/data_{data_num}_{cost:.2f}.csv', sep = ' ', names = [i for i in range(29)])\n",
    "        X = X.sample(200)\n",
    "    elif DATASET == 'Weather':\n",
    "        X = pd.read_csv(f'{ROOT_DIR}/data/{DATASET}/data_{data_num}_{cost:.2f}.csv', sep = ' ', names = [i for i in range(124)])\n",
    "        X = X.sample(n=1000)\n",
    "    ##get X and label\n",
    "    y = X.iloc[:,-1]\n",
    "    X = X.iloc[:,:-1]\n",
    "    return X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(DATASET, c, n, architecture):\n",
    "    X1, y1 = loadData(DATASET, 1, c)\n",
    "    X2, y2 = loadData(DATASET, 2, c)\n",
    "    model, criterion, optimizer, lr_scheduler = createModel(n)\n",
    "    dataloader = dp.DataPreprocessor(DATASET, BATCH_SIZE)\n",
    "    if architecture == 'single':\n",
    "        train_loader, val_loader, test_loader = dataloader.preprocess(X1, y1)\n",
    "    elif architecture == 'joint':\n",
    "        train_loader, val_loader, test_loader = dataloader.preprocess_joint(X1, y1, X2, y2)\n",
    "    # Training hyperparameters\n",
    "    PATIENCE = 5 \n",
    "    \n",
    "    # Train loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stop = False\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            outputs = model(x)\n",
    "            if DATASET in SQUEEZE:\n",
    "                y = y.unsqueeze(1)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            train_loss += loss.item() \n",
    "        train_loss /= len(train_loader)\n",
    "        # Validation\n",
    "        model.eval() \n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                outputs = model(x)\n",
    "                if DATASET in SQUEEZE:\n",
    "                    y = y.unsqueeze(1)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        # Log\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        N = 5\n",
    "        moving_avg_val_loss = sum(val_losses[-N:]) / N\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            counter = 0 \n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            if val_loss > moving_avg_val_loss:\n",
    "                counter += 1\n",
    "                if counter >= PATIENCE:\n",
    "                    early_stop = True\n",
    "                    break\n",
    "\n",
    "\n",
    "    #Test\n",
    "    test_loss = 0\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions_list = []\n",
    "        true_labels_list = []\n",
    "        test_loss = 0\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            predictions = best_model(X)\n",
    "            if DATASET in SQUEEZE:\n",
    "                y = y.unsqueeze(1)\n",
    "            loss = criterion(predictions, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Storing the predictions and true labels\n",
    "            predictions_list.extend(predictions.cpu().numpy())\n",
    "            true_labels_list.extend(y.cpu().numpy())\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        # Converting the lists to numpy arrays\n",
    "        predictions_array = np.array(predictions_list)\n",
    "        true_labels_array = np.array(true_labels_list)\n",
    "        # Calculating the AUC\n",
    "        metric_assess = get_metric(DATASET)\n",
    "        score = metric_assess(true_labels_array, predictions_array)\n",
    "    return best_model, score, train_losses, val_losses, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelRuns(DATASET, c, n, architecture):\n",
    "    scores = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    test_loss_list = []\n",
    "\n",
    "    for run in range(RUNS):\n",
    "        best_model, score, train_losses, val_losses, test_loss = runModel(DATASET, c, n, architecture)\n",
    "        scores.append(score)\n",
    "        train_loss_list.append(train_losses)\n",
    "        val_loss_list.append(val_losses)\n",
    "        test_loss_list.append(test_loss)\n",
    "    return scores, train_loss_list, val_loss_list, test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(data, alpha=0.95):\n",
    "    median = np.median(data)\n",
    "    bs_reps = bootstrap(np.array(data).reshape(1,-1), statistic=np.mean, n_resamples=1000)\n",
    "    ci = bs_reps.confidence_interval[0:2]\n",
    "    return median, ci[0], ci[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAnalysis(DATASET, costs, n):\n",
    "    results_scores = {}\n",
    "    results_train_losses = {}\n",
    "    results_val_losses = {}\n",
    "    for c in costs:\n",
    "        results_scores[c] = {}\n",
    "        results_train_losses[c] = {}\n",
    "        results_val_losses[c] = {}\n",
    "        for architecture in ['single', 'joint']:\n",
    "            scores, train_loss_list, val_loss_list, test_loss_list = modelRuns(DATASET, c, n, architecture)\n",
    "            results_scores[c][architecture] = scores\n",
    "            results_train_losses[c][architecture] = train_loss_list\n",
    "            results_val_losses[c][architecture] = val_loss_list\n",
    "\n",
    "    with open(f'{ROOT_DIR}/results/{DATASET}_scores.pkl', 'wb') as f:\n",
    "        pickle.dump(results_scores, f)\n",
    "    \n",
    "    with open(f'{ROOT_DIR}/results/{DATASET}_train_losses.pkl', 'wb') as f:\n",
    "        pickle.dump(results_train_losses, f)\n",
    "\n",
    "    with open(f'{ROOT_DIR}/results/{DATASET}_val_losses.pkl', 'wb') as f:\n",
    "        pickle.dump(results_val_losses, f)\n",
    "\n",
    "    return results_scores, results_train_losses, results_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(auc_scores,train_loss_list, val_loss_list, test_loss_list, data_type):    \n",
    "    # Plotting Train Losses\n",
    "    plt.figure(figsize = (5,3))\n",
    "    # Determine the maximum length\n",
    "    train_loss_list = [t[5:] for t in train_loss_list]\n",
    "    val_loss_list = [v[5:] for v in val_loss_list]\n",
    "    max_length = max(max(len(t) for t in train_loss_list), max(len(v) for v in val_loss_list))\n",
    "    # Pad the shorter lists with np.nan\n",
    "    train_losses_padded = [np.pad(t, (0, max_length - len(t)), 'constant', constant_values=np.nan) for t in train_loss_list]\n",
    "    val_losses_padded = [np.pad(v, (0, max_length - len(v)), 'constant', constant_values=np.nan) for v in val_loss_list]\n",
    "    train_losses = pd.DataFrame(train_losses_padded)\n",
    "    val_losses = pd.DataFrame(val_losses_padded)\n",
    "    sns.lineplot(train_losses.mean(axis = 0), label = 'Train', alpha = 0.5)\n",
    "    sns.lineplot(val_losses.mean(axis = 0), label = 'Val', alpha = 0.5)\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "                super(Feedforward, self).__init__()\n",
    "                self.input_size = input_size\n",
    "                self.hidden_size = [56, 6]\n",
    "                self.fc = torch.nn.Sequential(nn.Linear(self.input_size, self.hidden_size[0]),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Dropout(0.3),\n",
    "                                                nn.Linear(self.hidden_size[0], self.hidden_size[1]),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(self.hidden_size[1], 1))\n",
    "                self.sigmoid = torch.nn.Sigmoid()\n",
    "                for layer in self.fc:\n",
    "                        if isinstance(layer, nn.Linear):\n",
    "                                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "                                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "        def forward(self, x):\n",
    "                output = self.fc(x)\n",
    "                output = self.sigmoid(output)\n",
    "                return output\n",
    "        \n",
    "def createModel(n):\n",
    "    model = Feedforward(n)\n",
    "    model.to(DEVICE)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "    lr_scheduler = ExponentialLR(optimizer, gamma=0.8)\n",
    "    return model, criterion, optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "BATCH_SIZE = 2000\n",
    "RUNS = 100\n",
    "DATASET = 'Synthetic'\n",
    "METRIC_TEST = 'AUC'\n",
    "LEARNING_RATE = 5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = [0.03, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60]\n",
    "n = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_scores, results_train_losses, results_val_losses = runAnalysis(DATASET, costs, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "                super(Feedforward, self).__init__()\n",
    "                self.input_size = input_size\n",
    "                self.hidden_size  = [128, 56, 12]\n",
    "                self.fc = nn.Sequential(\n",
    "                        nn.Linear(self.input_size, self.hidden_size[0]),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.3),\n",
    "                        nn.Linear(self.hidden_size[0], self.hidden_size[1]),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.3),\n",
    "                        nn.Linear(self.hidden_size[1], self.hidden_size[2]),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(self.hidden_size[2], 1)\n",
    "                )\n",
    "                for layer in self.fc:\n",
    "                    if isinstance(layer, nn.Linear):\n",
    "                            nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "                            nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "                self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x):\n",
    "                output = self.fc(x)\n",
    "                output = self.sigmoid(output)\n",
    "                return output\n",
    "        \n",
    "def createModel(n):\n",
    "    model = Feedforward(28)\n",
    "    model.to(DEVICE)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "    lr_scheduler = ExponentialLR(optimizer, gamma=0.8)\n",
    "    return model, criterion, optimizer, lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 2000\n",
    "RUNS = 10\n",
    "DATASET = 'Credit'\n",
    "METRIC_TEST = 'AUPRC'\n",
    "LEARNING_RATE = 8e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = [0.12, 0.23, 0.30, 0.41]\n",
    "c = costs[0]\n",
    "n = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_scores, results_train_losses, results_val_losses = runAnalysis(DATASET, costs, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "                super(Feedforward, self).__init__()\n",
    "                self.input_size = input_size\n",
    "                self.hidden_size  = [300,150,30]\n",
    "                self.fc = nn.Sequential(\n",
    "                        nn.Linear(self.input_size, self.hidden_size[0]),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.3),\n",
    "                        nn.Linear(self.hidden_size[0], self.hidden_size[1]),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.3),\n",
    "                        nn.Linear(self.hidden_size[1], (self.hidden_size[2])),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(self.hidden_size[2], 1)\n",
    "                )\n",
    "                for layer in self.fc:\n",
    "                        if isinstance(layer, nn.Linear):\n",
    "                                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "                                nn.init.constant_(layer.bias, 0)\n",
    "        def forward(self, x):\n",
    "                output = self.fc(x)\n",
    "                return output\n",
    "        \n",
    "def createModel(n):\n",
    "    model = Feedforward(n)\n",
    "    model.to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "    lr_scheduler = ExponentialLR(optimizer, gamma=0.8)\n",
    "    return model, criterion, optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 4000\n",
    "RUNS = 100\n",
    "DATASET = 'Weather'\n",
    "METRIC_TEST = 'R2'\n",
    "LEARNING_RATE = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = [0.11, 0.19, 0.30, 0.40, 0.48]\n",
    "c = costs[0]\n",
    "n = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_scores, results_train_losses, results_val_losses = runAnalysis(DATASET, costs, n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
